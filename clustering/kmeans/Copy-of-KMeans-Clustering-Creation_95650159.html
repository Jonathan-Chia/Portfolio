<!DOCTYPE html>
<html>
    <head>
        <title>Chia, Jonathan : Copy of KMeans Clustering Creation</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">Chia, Jonathan</a></span>
                            </li>
                                                    <li>
                                <span><a href="Productionized-Articles_95650120.html">Productionized Articles</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Chia, Jonathan : Copy of KMeans Clustering Creation
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                        
        
    
        
    
        
        
            Created by <span class='author'> Chia, Jonathan</span> on Apr 09, 2022
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <p><em>Notes on cluster creation for behavioral customer segmentation</em></p><p><br/></p><h1 id="CopyofKMeansClusteringCreation-TheGoal">The Goal</h1><p>Create distinguishable clusters of customers. Each cluster of customers should have different behaviors. </p><p><br/></p><p>Because there are so many different behaviors, segmenting customers manually would be very complex. Let's say we segment them by what products they purchase. We may end up with a lot of easy to name groups, but the problem we would find is that bella customers and color silver customer might have very similar behaviors because these brands appeal to new customers. Not to mention how do you decide how to split these groups up? If you purchase 50% in bella, you are in the bella group? What about 60%? </p><p>The reason we use machine learning is because the algorithm can use math to find groups that are distinct and separate from each other in terms of behaviors. Kmeans essentially plots all the different customers in a multi-dimensional space, and then finds these groups.</p><p><br/></p><p>One big problem we have is that we have so many variables that we start to have dimensionality problems (curse of dimensionality); there are some variables that are not very useful, some variables that create noise, and these extra variables make it much harder for the algorithm to work efficiently and accurately.</p><p><br/></p><h1 id="CopyofKMeansClusteringCreation-PCA">PCA</h1><p>Principal Component Analysis helps to solve the problems we face with multi-dimensional data. It can reduce noise, reduce non-essential variables, and still preserve most of the variance. </p><h3 id="CopyofKMeansClusteringCreation-DeterminingifPCAisneeded">Determining if PCA is needed</h3><p>I don't remember what the best way is to decide, but you can run Kmeans with PCA and without it and then compare the results. It all depends on your data and your problem. In our case, we tried it with PCA and we liked the results we got from it.</p><h3 id="CopyofKMeansClusteringCreation-NormalizingbeforerunningPCA"><strong>Normalizing before running PCA</strong></h3><p>PCA decides on dimensionality reduction based on variance. Make sure everything is scaled from 0 to 1 so that each variable is weighted equally</p><h3 id="CopyofKMeansClusteringCreation-FindingtherightnumberofPCAcolumnstouse">Finding the right number of PCA columns to use</h3><p>Permutation Test:</p><p>Check to make sure the variance in each column is not due to random chance</p><p>Null hypothesis: simulate the null hypothesis by shuffling columns so that each row (each point in the dimensional space) is random. Thus the PCA we get is now a random null distribution for if the data is random.</p><p>Alternative hypothesis: If we reject the null hypothesis then we know that the PCA we found was from real variance not random variance</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">find_best_pcs &lt;- function(df,N_perm) {  
	print(&quot;Finding PCA Variance&quot;)  
	PC &lt;- prcomp(df, center=TRUE, scale=FALSE)  
	expl_var &lt;- PC$sdev^2/sum(PC$sdev^2)  # converting the standard deviation back to the variance
	print(&quot;Conducting Permutation Test for best PCs&quot;)  

	expl_var_perm &lt;- matrix(NA, ncol = length(PC$sdev), nrow = N_perm)  
	for(i in 1:N_perm) {    	
		print(paste(&quot;Permutation Number&quot;,i))    
		data_perm &lt;- apply(df,2,sample)    # shuffle the data in each column
		PC_perm &lt;- prcomp(data_perm, center=TRUE, scale=FALSE)    # apply PCA to the data_perm (this data represents the null distribution)
		expl_var_perm[i,] &lt;- PC_perm$sdev^2/sum(PC_perm$sdev^2)  # converting from sd to variance
	} 

	pval &lt;- apply(t(expl_var_perm) &gt;= expl_var,1,sum) / N_perm  # running a hypothesis test comparing the variance from the actual data versus the variance in the permuted (null distribution) data
	out &lt;- list(    &quot;best_pc&quot; = head(which(pval&gt;=0.05),1)-1,    &quot;expl_var&quot; = expl_var,    &quot;expl_var_perm&quot; = expl_var_perm,    &quot;pval_perm&quot; = pval)    

	return(out)
}</pre>
</div></div><p><br/></p><p><br/></p><h1 id="CopyofKMeansClusteringCreation-K-Means">K-Means</h1><p>See the below link for a visual explanation of the algorithm:</p><p style="margin-left: 0.0px;text-align: left;"><a style="text-decoration: none;" href="https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175" class="external-link" rel="nofollow">https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175</a></p><h3 id="CopyofKMeansClusteringCreation-FindingtherightK">Finding the right K</h3><p>Remember, the goal is to create distinguishable clusters. The more clusters we can make the better (so a higher k is generally better), but we have to be careful. With a higher number of groups, the groups can become less distinguishable. For example, let's say we have 4 groups, and groups 1 and 2 are pretty close to each other. Let's say we run it with 5 groups, and the 5th group takes some from group 1 and some from group 2. Now groups 1, 2, and 5 are all really close, with 5 bridging 1 and 2. These groups may not be as distinguishable now. </p><h3 id="CopyofKMeansClusteringCreation-Testingifthealgorithmmadedistinguishableenoughclusters">Testing if the algorithm made distinguishable enough clusters</h3><p>If the clusters are distinguishable enough, a basic algorithm such as a decision tree should be able to see clustered data, learn the differences in behavior, and then 'replicate' the clustering algorithm.</p><p><br/></p><p>Let's look at the code for a better explanation of this 'replication':</p><ol><li>First we cluster the data and assign labels to all the customers</li><li>Then we split the data into a training and testing set</li><li>Finally, we run a decision tree on the training data</li></ol><p>If the clusters are distinguishable enough, the decision tree should be able to learn the differences in the clusters. To check if the decision tree learned well, we then show the test data (without the cluster labels) to the decision tree. The decision tree then assigns the labels to the test data.</p><p>We then use the confusion matrix to compare the test data's actual labels versus the decision tree's assigned labels.</p><p><br/></p><p>If accuracy is very high, then we know that each cluster was different enough that the decision tree didn't get confused when assigning cluster labels to customers.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: py; gutter: false; theme: Confluence" data-theme="Confluence">cluster_comparison &lt;- function(temp,n_clusts) {   
	# 1. create temp data and cluster 
	print(&quot;Clustering Data&quot;) 
	temp$temp_clust &lt;- (temp %&gt;% kmeans(n_clusts))$cluster 

	# 2. Train Test split
	 print(&quot;TrainTest Split&quot;) 
	train &lt;- temp[tts(nrow(temp),.8,100),] 
	test &lt;- temp[-tts(nrow(temp),.8,100),]   
	

	# 3. Train and Predict 
	print(&quot;Rpartition&quot;) 
	rpm &lt;- rpart(as.factor(temp_clust) ~ .,train) 
	pp &lt;- predict(rpm,newdata = select(test,-temp_clust)) 
	pp &lt;- sapply(1:nrow(test),function(x) which.max(pp[x,])) 
	pp &lt;- confusionMatrix(as.factor(pp),as.factor(test$temp_clust)) 
	#sum(diag(pp$table)/sum(pp$table))}
</pre>
</div></div><p><br/></p><p><br/></p><p><br/></p>
                    </div>

                                        
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on Apr 09, 2022 16:54</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
